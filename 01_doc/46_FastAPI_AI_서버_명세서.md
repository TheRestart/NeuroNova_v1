# FastAPI AI ì„œë²„ ëª…ì„¸ì„œ

**ë¬¸ì„œ ID**: 46_FastAPI_AI_ì„œë²„_ëª…ì„¸ì„œ
**ë²„ì „**: v3.0 (âœ… í™•ì •)
**ìµœì¢… ìˆ˜ì •ì¼**: 2026-01-02
**ì‘ì„±ì**: Claude AI (Sonnet 4.5)
**ìƒíƒœ**: âœ… í™•ì •

---

## ğŸ“‹ Changelog

### v3.0 (2026-01-02) - FastAPI í†µì‹  ëª…ì„¸ í™•ì •
- âœ… **Celery â†” FastAPI HTTP REST API ëª…ì„¸ í™•ì •**
- âœ… **OpenAPI 3.0 Swagger ìŠ¤í™ ì‘ì„±**
- âœ… **HTJ2K ë³€í™˜ ì±…ì„ ëª…í™•í™”** (Orthanc Pluginìœ¼ë¡œ ì´ê´€)
- âœ… **AI ì¶”ë¡  ì—”ë“œí¬ì¸íŠ¸ ìƒì„¸ ì •ì˜** (`/inference`, `/models`, `/health`)
- âœ… **ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ëª…í™•í™”** (Pydantic ëª¨ë¸)
- âœ… **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê¸°ì¤€ ìˆ˜ë¦½**
- âœ… **Docker ë°°í¬ ì„¤ì • í¬í•¨**

### ë³€ê²½ ì´ìœ 
- Celery Workerì™€ FastAPI ê°„ í†µì‹  ë°©ë²•ì´ ë¶ˆëª…í™•í–ˆìŒ (ì•„í‚¤í…ì²˜ ì ê²€ ë³´ê³ ì„œ High-2)
- HTJ2K ë³€í™˜ ì±…ì„ì´ FastAPI/Celery ê°„ ëª¨í˜¸í–ˆìŒ (v3.0ì—ì„œ Orthanc Pluginìœ¼ë¡œ ì´ê´€)
- AI ì¶”ë¡  APIì˜ í‘œì¤€í™”ëœ ìŠ¤í‚¤ë§ˆ í•„ìš”

---

## 1. ì‹œìŠ¤í…œ ê°œìš”

### 1.1 FastAPI AI ì„œë²„ì˜ ì—­í• 

FastAPI AI ì„œë²„ëŠ” **NeuroNova CDSSì˜ AI ì¶”ë¡  ì—”ì§„**ìœ¼ë¡œ, ë‹¤ìŒ ì±…ì„ì„ ê°€ì§‘ë‹ˆë‹¤:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FastAPI AI ì„œë²„                          â”‚
â”‚                                                               â”‚
â”‚  [ì±…ì„ ë²”ìœ„]                                                  â”‚
â”‚  1. AI ëª¨ë¸ ì¶”ë¡  (CT/MRI/X-Ray ì˜ìƒ ë¶„ì„)                    â”‚
â”‚  2. DICOM ë©”íƒ€ë°ì´í„° íŒŒì‹± ë° ê²€ì¦                            â”‚
â”‚  3. NumPy ë°°ì—´ ì „ì²˜ë¦¬ (ì •ê·œí™”, ë¦¬ì‚¬ì´ì§•)                     â”‚
â”‚  4. ì¶”ë¡  ê²°ê³¼ í›„ì²˜ë¦¬ (í™•ë¥  â†’ ì§„ë‹¨ ì½”ë“œ ë§¤í•‘)                â”‚
â”‚  5. ëª¨ë¸ ìƒíƒœ ëª¨ë‹ˆí„°ë§ (health check, metrics)               â”‚
â”‚                                                               â”‚
â”‚  [ì±…ì„ ì™¸ ë²”ìœ„]                                               â”‚
â”‚  âœ— HTJ2K ë³€í™˜ (Orthanc Pluginì´ ë‹´ë‹¹)                        â”‚
â”‚  âœ— DICOM ì €ì¥ (Orthancê°€ ë‹´ë‹¹)                               â”‚
â”‚  âœ— ì¥ê¸° ì‹¤í–‰ ì‘ì—… ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (Celeryê°€ ë‹´ë‹¹)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 í†µì‹  ì•„í‚¤í…ì²˜

```mermaid
sequenceDiagram
    participant User as ì‚¬ìš©ì (Doctor)
    participant Django as Django REST API
    participant Celery as Celery Worker
    participant FastAPI as FastAPI AI ì„œë²„
    participant Orthanc as Orthanc PACS

    User->>Django: POST /api/ai/inference-request/
    Django->>Celery: analyze_dicom_image.delay(study_uid)
    Celery-->>Django: Task ID ë°˜í™˜
    Django-->>User: 202 Accepted (task_id)

    Note over Celery,FastAPI: ë¹„ë™ê¸° ì¶”ë¡  ì‹œì‘

    Celery->>Orthanc: GET /studies/{study_uid}/instances
    Orthanc-->>Celery: DICOM íŒŒì¼ ëª©ë¡

    Celery->>Orthanc: GET /instances/{instance_id}/file
    Orthanc-->>Celery: DICOM ì›ë³¸ (HTJ2K ì´ë¯¸ ë³€í™˜ë¨)

    Celery->>Celery: pydicomìœ¼ë¡œ í”½ì…€ ë°ì´í„° ì¶”ì¶œ

    Celery->>FastAPI: POST /api/v1/inference (HTTP)
    Note right of Celery: JSON payload:<br/>{<br/>  "image_array": [...],<br/>  "modality": "CT",<br/>  "metadata": {...}<br/>}

    FastAPI->>FastAPI: AI ëª¨ë¸ ì¶”ë¡ 
    FastAPI-->>Celery: 200 OK (ê²°ê³¼ JSON)

    Celery->>Django: APIë¡œ ê²°ê³¼ ì €ì¥
    Django-->>User: WebSocket ì•Œë¦¼ (ì¶”ë¡  ì™„ë£Œ)
```

**í•µì‹¬ ì›ì¹™:**
1. **Celeryê°€ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°**: DICOM ë‹¤ìš´ë¡œë“œ, ì „ì²˜ë¦¬, FastAPI í˜¸ì¶œ, ê²°ê³¼ ì €ì¥
2. **FastAPIëŠ” Stateless ì¶”ë¡ ê¸°**: HTTP ìš”ì²­ ë°›ì•„ AI ëª¨ë¸ ì‹¤í–‰ í›„ ì‘ë‹µ
3. **Orthancê°€ HTJ2K ë³€í™˜**: Celery/FastAPIëŠ” ë³€í™˜ëœ DICOMë§Œ ì²˜ë¦¬

---

## 2. OpenAPI 3.0 ëª…ì„¸ì„œ

### 2.1 ê¸°ë³¸ ì •ë³´

```yaml
openapi: 3.0.3
info:
  title: NeuroNova AI Inference API
  version: 3.0.0
  description: |
    ì˜ë£Œ ì˜ìƒ AI ì¶”ë¡  ì„œë¹„ìŠ¤ API
    - CT, MRI, X-Ray ì˜ìƒ ë¶„ì„
    - DICOM ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
    - RESTful JSON ì¸í„°í˜ì´ìŠ¤
  contact:
    name: NeuroNova CDSS Team
    email: dev@neuronova.example.com
  license:
    name: Proprietary

servers:
  - url: http://fastapi-ai:8000/api/v1
    description: Docker Compose ë‚´ë¶€ (Production)
  - url: http://localhost:8000/api/v1
    description: ë¡œì»¬ ê°œë°œ í™˜ê²½

tags:
  - name: Inference
    description: AI ëª¨ë¸ ì¶”ë¡  ì—”ë“œí¬ì¸íŠ¸
  - name: Models
    description: ëª¨ë¸ ê´€ë¦¬ ë° ì •ë³´ ì¡°íšŒ
  - name: Health
    description: ì„œë²„ ìƒíƒœ í™•ì¸
```

### 2.2 ì—”ë“œí¬ì¸íŠ¸ ëª©ë¡

| Method | Path | ì„¤ëª… | ì¸ì¦ |
|--------|------|------|------|
| `POST` | `/inference` | AI ì¶”ë¡  ì‹¤í–‰ | Internal (Celeryë§Œ í˜¸ì¶œ) |
| `GET` | `/models` | ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ | None |
| `GET` | `/models/{model_id}` | íŠ¹ì • ëª¨ë¸ ìƒì„¸ ì •ë³´ | None |
| `GET` | `/health` | ì„œë²„ Health Check | None |
| `GET` | `/metrics` | Prometheus ë©”íŠ¸ë¦­ | None |

---

## 3. API ì—”ë“œí¬ì¸íŠ¸ ìƒì„¸

### 3.1 POST `/inference` - AI ì¶”ë¡  ì‹¤í–‰

**ëª©ì **: DICOM ì˜ìƒ ë°ì´í„°ì— ëŒ€í•œ AI ëª¨ë¸ ì¶”ë¡ 

#### ìš”ì²­ ìŠ¤í‚¤ë§ˆ

```json
{
  "image_array": [[...]],
  "modality": "CT",
  "metadata": {
    "StudyInstanceUID": "1.2.840.113619.2.55.3...",
    "SeriesInstanceUID": "1.2.840.113619.2.55.3...",
    "SOPInstanceUID": "1.2.840.113619.2.55.3...",
    "PatientID": "P-2024-001",
    "StudyDescription": "Brain CT",
    "BodyPartExamined": "BRAIN",
    "SliceThickness": 5.0,
    "KVP": 120,
    "WindowCenter": 40,
    "WindowWidth": 80
  },
  "model_id": "brain_hemorrhage_v2",
  "options": {
    "return_heatmap": true,
    "confidence_threshold": 0.7
  }
}
```

**Pydantic ëª¨ë¸ (FastAPI)**:
```python
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from enum import Enum

class ModalityEnum(str, Enum):
    CT = "CT"
    MR = "MR"
    XR = "XR"  # X-Ray
    DX = "DX"  # Digital Radiography

class DICOMMetadata(BaseModel):
    StudyInstanceUID: str
    SeriesInstanceUID: str
    SOPInstanceUID: str
    PatientID: str
    StudyDescription: Optional[str] = None
    BodyPartExamined: Optional[str] = None
    SliceThickness: Optional[float] = None
    KVP: Optional[float] = None
    WindowCenter: Optional[float] = None
    WindowWidth: Optional[float] = None

class InferenceOptions(BaseModel):
    return_heatmap: bool = Field(default=False, description="CAM íˆíŠ¸ë§µ ë°˜í™˜ ì—¬ë¶€")
    confidence_threshold: float = Field(default=0.5, ge=0.0, le=1.0)
    batch_size: int = Field(default=1, ge=1, le=16)

class InferenceRequest(BaseModel):
    image_array: List[List[List[float]]]  # 3D array: [slices, height, width]
    modality: ModalityEnum
    metadata: DICOMMetadata
    model_id: Optional[str] = Field(default=None, description="ë¯¸ì§€ì • ì‹œ ìë™ ì„ íƒ")
    options: InferenceOptions = InferenceOptions()

    class Config:
        schema_extra = {
            "example": {
                "image_array": [[[0.5, 0.6, ...], [...]], ...],
                "modality": "CT",
                "metadata": {
                    "StudyInstanceUID": "1.2.840.113619.2.55.3.12345",
                    "SeriesInstanceUID": "1.2.840.113619.2.55.3.12346",
                    "SOPInstanceUID": "1.2.840.113619.2.55.3.12347",
                    "PatientID": "P-2024-001",
                    "BodyPartExamined": "BRAIN"
                },
                "model_id": "brain_hemorrhage_v2"
            }
        }
```

#### ì‘ë‹µ ìŠ¤í‚¤ë§ˆ

**200 OK (ì„±ê³µ)**:
```json
{
  "inference_id": "inf_2024_001_abc123",
  "model_id": "brain_hemorrhage_v2",
  "model_version": "2.1.0",
  "predictions": [
    {
      "class_name": "Subarachnoid Hemorrhage",
      "class_code": "ICD10:I60.9",
      "confidence": 0.89,
      "bounding_box": {
        "x": 120,
        "y": 150,
        "width": 80,
        "height": 60
      }
    },
    {
      "class_name": "Normal",
      "class_code": "ICD10:Z00.00",
      "confidence": 0.11
    }
  ],
  "heatmap": {
    "format": "base64_png",
    "data": "iVBORw0KGgoAAAANSUhEUgAA..."
  },
  "metadata": {
    "inference_time_ms": 456.7,
    "preprocessing_time_ms": 123.4,
    "model_device": "cuda:0",
    "input_shape": [1, 512, 512],
    "timestamp": "2026-01-02T10:30:45.123456Z"
  }
}
```

**Pydantic ì‘ë‹µ ëª¨ë¸**:
```python
class Prediction(BaseModel):
    class_name: str
    class_code: str = Field(..., description="ICD-10 ë˜ëŠ” SNOMED CT ì½”ë“œ")
    confidence: float = Field(..., ge=0.0, le=1.0)
    bounding_box: Optional[Dict[str, int]] = None

class Heatmap(BaseModel):
    format: str = Field(default="base64_png")
    data: str

class InferenceMetadata(BaseModel):
    inference_time_ms: float
    preprocessing_time_ms: float
    model_device: str
    input_shape: List[int]
    timestamp: str

class InferenceResponse(BaseModel):
    inference_id: str
    model_id: str
    model_version: str
    predictions: List[Prediction]
    heatmap: Optional[Heatmap] = None
    metadata: InferenceMetadata
```

**422 Unprocessable Entity (ê²€ì¦ ì‹¤íŒ¨)**:
```json
{
  "detail": [
    {
      "loc": ["body", "modality"],
      "msg": "value is not a valid enumeration member; permitted: 'CT', 'MR', 'XR', 'DX'",
      "type": "type_error.enum"
    }
  ]
}
```

**500 Internal Server Error (ì¶”ë¡  ì‹¤íŒ¨)**:
```json
{
  "error": "ModelInferenceError",
  "message": "CUDA out of memory",
  "detail": "Failed to allocate 2.5GB on GPU",
  "model_id": "brain_hemorrhage_v2",
  "timestamp": "2026-01-02T10:30:45.123456Z"
}
```

#### FastAPI ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„ ì˜ˆì‹œ

```python
from fastapi import FastAPI, HTTPException, status
from fastapi.responses import JSONResponse
import torch
import numpy as np
import logging
from datetime import datetime
import uuid

app = FastAPI(title="NeuroNova AI Inference API", version="3.0.0")
logger = logging.getLogger(__name__)

# ëª¨ë¸ ë¡œë”© (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ)
models = {}

@app.on_event("startup")
async def load_models():
    """ì„œë²„ ì‹œì‘ ì‹œ AI ëª¨ë¸ ë¡œë“œ"""
    logger.info("Loading AI models...")

    # ì˜ˆì‹œ: Brain Hemorrhage Detection Model
    models['brain_hemorrhage_v2'] = {
        'model': torch.jit.load('/models/brain_hemorrhage_v2.pt'),
        'version': '2.1.0',
        'modality': 'CT',
        'body_part': 'BRAIN'
    }

    # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ ì´ë™
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    for model_id, model_info in models.items():
        model_info['model'].to(device)
        model_info['model'].eval()
        logger.info(f"Model {model_id} loaded on {device}")

@app.post("/api/v1/inference", response_model=InferenceResponse)
async def inference(request: InferenceRequest):
    """
    AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰

    **Celery Workerì—ì„œë§Œ í˜¸ì¶œ**ë˜ëŠ” ë‚´ë¶€ APIì…ë‹ˆë‹¤.
    """
    start_time = datetime.utcnow()
    inference_id = f"inf_{datetime.now().strftime('%Y%m%d')}_{uuid.uuid4().hex[:8]}"

    try:
        # 1. ëª¨ë¸ ì„ íƒ
        model_id = request.model_id
        if not model_id:
            # ìë™ ì„ íƒ ë¡œì§ (Modality + BodyPart ê¸°ë°˜)
            model_id = select_model_auto(request.modality, request.metadata.BodyPartExamined)

        if model_id not in models:
            raise HTTPException(
                status_code=404,
                detail=f"Model '{model_id}' not found"
            )

        model_info = models[model_id]

        # 2. ì…ë ¥ ì „ì²˜ë¦¬
        preprocess_start = datetime.utcnow()
        input_tensor = preprocess_image(
            np.array(request.image_array),
            target_size=(512, 512)
        )
        preprocess_time = (datetime.utcnow() - preprocess_start).total_seconds() * 1000

        # 3. ì¶”ë¡  ì‹¤í–‰
        device = next(model_info['model'].parameters()).device
        with torch.no_grad():
            input_tensor = input_tensor.to(device)
            output = model_info['model'](input_tensor)
            probabilities = torch.softmax(output, dim=1).cpu().numpy()[0]

        inference_time = (datetime.utcnow() - start_time).total_seconds() * 1000

        # 4. í›„ì²˜ë¦¬
        predictions = postprocess_predictions(
            probabilities,
            model_id,
            threshold=request.options.confidence_threshold
        )

        # 5. íˆíŠ¸ë§µ ìƒì„± (ì˜µì…˜)
        heatmap = None
        if request.options.return_heatmap:
            heatmap = generate_cam_heatmap(model_info['model'], input_tensor)

        # 6. ì‘ë‹µ ìƒì„±
        return InferenceResponse(
            inference_id=inference_id,
            model_id=model_id,
            model_version=model_info['version'],
            predictions=predictions,
            heatmap=heatmap,
            metadata=InferenceMetadata(
                inference_time_ms=inference_time,
                preprocessing_time_ms=preprocess_time,
                model_device=str(device),
                input_shape=list(input_tensor.shape),
                timestamp=datetime.utcnow().isoformat() + "Z"
            )
        )

    except torch.cuda.OutOfMemoryError as e:
        logger.error(f"CUDA OOM: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "CUDAOutOfMemory",
                "message": "GPU ë©”ëª¨ë¦¬ ë¶€ì¡±",
                "model_id": model_id
            }
        )

    except Exception as e:
        logger.error(f"Inference error: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail={
                "error": "ModelInferenceError",
                "message": str(e),
                "model_id": model_id
            }
        )

def preprocess_image(image_array: np.ndarray, target_size=(512, 512)) -> torch.Tensor:
    """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    # ë¦¬ì‚¬ì´ì§•
    from skimage.transform import resize
    resized = resize(image_array, target_size, anti_aliasing=True)

    # ì •ê·œí™” (Hounsfield Unit â†’ [0, 1])
    normalized = (resized - resized.min()) / (resized.max() - resized.min() + 1e-8)

    # Tensor ë³€í™˜ [batch, channel, height, width]
    tensor = torch.from_numpy(normalized).float().unsqueeze(0).unsqueeze(0)

    return tensor

def postprocess_predictions(probabilities: np.ndarray, model_id: str, threshold=0.5):
    """í™•ë¥  â†’ ì§„ë‹¨ ì½”ë“œ ë§¤í•‘"""
    # ì˜ˆì‹œ: Brain Hemorrhage 5-class
    class_mapping = {
        0: {"name": "Normal", "code": "ICD10:Z00.00"},
        1: {"name": "Epidural Hemorrhage", "code": "ICD10:I62.1"},
        2: {"name": "Subdural Hemorrhage", "code": "ICD10:I62.0"},
        3: {"name": "Subarachnoid Hemorrhage", "code": "ICD10:I60.9"},
        4: {"name": "Intraventricular Hemorrhage", "code": "ICD10:I61.5"}
    }

    predictions = []
    for idx, prob in enumerate(probabilities):
        if prob >= threshold or idx == probabilities.argmax():  # ìµœì†Œ 1ê°œëŠ” í¬í•¨
            predictions.append(Prediction(
                class_name=class_mapping[idx]['name'],
                class_code=class_mapping[idx]['code'],
                confidence=float(prob)
            ))

    # í™•ë¥  ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    predictions.sort(key=lambda x: x.confidence, reverse=True)
    return predictions

def select_model_auto(modality: str, body_part: str) -> str:
    """Modality + BodyPart ê¸°ë°˜ ìë™ ëª¨ë¸ ì„ íƒ"""
    mapping = {
        ("CT", "BRAIN"): "brain_hemorrhage_v2",
        ("CT", "CHEST"): "lung_nodule_v1",
        ("MR", "BRAIN"): "brain_tumor_v3",
        ("XR", "CHEST"): "chest_xray_v2"
    }

    model_id = mapping.get((modality, body_part))
    if not model_id:
        raise HTTPException(
            status_code=400,
            detail=f"No model available for {modality} + {body_part}"
        )
    return model_id

def generate_cam_heatmap(model, input_tensor):
    """Grad-CAM íˆíŠ¸ë§µ ìƒì„±"""
    # ê°„ë‹¨í•œ ì˜ˆì‹œ (ì‹¤ì œë¡œëŠ” Grad-CAM êµ¬í˜„ í•„ìš”)
    import base64
    from io import BytesIO
    from PIL import Image

    # Placeholder: ì‹¤ì œë¡œëŠ” Grad-CAM ì•Œê³ ë¦¬ì¦˜ ì ìš©
    heatmap_array = np.random.rand(512, 512) * 255
    img = Image.fromarray(heatmap_array.astype(np.uint8), mode='L')

    buffer = BytesIO()
    img.save(buffer, format='PNG')
    base64_data = base64.b64encode(buffer.getvalue()).decode('utf-8')

    return Heatmap(format="base64_png", data=base64_data)
```

---

### 3.2 GET `/models` - ëª¨ë¸ ëª©ë¡ ì¡°íšŒ

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
{
  "models": [
    {
      "model_id": "brain_hemorrhage_v2",
      "version": "2.1.0",
      "name": "Brain Hemorrhage Detection",
      "modality": "CT",
      "body_part": "BRAIN",
      "classes": [
        "Normal",
        "Epidural Hemorrhage",
        "Subdural Hemorrhage",
        "Subarachnoid Hemorrhage",
        "Intraventricular Hemorrhage"
      ],
      "accuracy": 0.94,
      "last_updated": "2025-12-15T00:00:00Z"
    },
    {
      "model_id": "lung_nodule_v1",
      "version": "1.5.0",
      "name": "Lung Nodule Detection",
      "modality": "CT",
      "body_part": "CHEST",
      "classes": ["Benign", "Malignant"],
      "accuracy": 0.91,
      "last_updated": "2025-11-20T00:00:00Z"
    }
  ]
}
```

**FastAPI êµ¬í˜„**:
```python
@app.get("/api/v1/models")
async def list_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
    model_list = []
    for model_id, info in models.items():
        model_list.append({
            "model_id": model_id,
            "version": info['version'],
            "modality": info['modality'],
            "body_part": info['body_part'],
            # ... ì¶”ê°€ ì •ë³´
        })
    return {"models": model_list}
```

---

### 3.3 GET `/health` - Health Check

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
{
  "status": "healthy",
  "timestamp": "2026-01-02T10:30:45.123456Z",
  "version": "3.0.0",
  "models_loaded": 4,
  "gpu_available": true,
  "gpu_memory_used_mb": 2048,
  "gpu_memory_total_mb": 8192
}
```

**FastAPI êµ¬í˜„**:
```python
@app.get("/api/v1/health")
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    gpu_available = torch.cuda.is_available()
    gpu_memory_used = 0
    gpu_memory_total = 0

    if gpu_available:
        gpu_memory_used = torch.cuda.memory_allocated(0) / 1024**2
        gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**2

    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "version": "3.0.0",
        "models_loaded": len(models),
        "gpu_available": gpu_available,
        "gpu_memory_used_mb": round(gpu_memory_used, 2),
        "gpu_memory_total_mb": round(gpu_memory_total, 2)
    }
```

---

## 4. Celery â†’ FastAPI í†µì‹  ì½”ë“œ

### 4.1 Celery Task êµ¬í˜„

**íŒŒì¼**: `NeuroNova_02_backend/ai_tasks/tasks.py`

```python
from celery import shared_task
import requests
import pydicom
import numpy as np
import logging
from typing import Dict, Any
from django.conf import settings

logger = logging.getLogger(__name__)

FASTAPI_BASE_URL = settings.FASTAPI_AI_URL  # "http://fastapi-ai:8000/api/v1"

@shared_task(bind=True, max_retries=3)
def analyze_dicom_image(self, study_instance_uid: str, model_id: str = None) -> Dict[str, Any]:
    """
    DICOM ì˜ìƒ AI ë¶„ì„ Celery Task

    Args:
        study_instance_uid: ë¶„ì„í•  Study UID
        model_id: ì‚¬ìš©í•  ëª¨ë¸ ID (Noneì´ë©´ ìë™ ì„ íƒ)

    Returns:
        ì¶”ë¡  ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    try:
        # 1. Orthancì—ì„œ DICOM ì¸ìŠ¤í„´ìŠ¤ ì¡°íšŒ
        logger.info(f"Fetching DICOM instances for Study: {study_instance_uid}")
        orthanc_url = settings.ORTHANC_URL  # "http://orthanc:8042"

        instances_response = requests.get(
            f"{orthanc_url}/studies/{study_instance_uid}/instances",
            auth=(settings.ORTHANC_USERNAME, settings.ORTHANC_PASSWORD)
        )
        instances_response.raise_for_status()
        instances = instances_response.json()

        if not instances:
            raise ValueError(f"No instances found for Study {study_instance_uid}")

        # 2. ì²« ë²ˆì§¸ ì¸ìŠ¤í„´ìŠ¤ ë‹¤ìš´ë¡œë“œ (ë©€í‹° ìŠ¬ë¼ì´ìŠ¤ëŠ” ì¶”í›„ í™•ì¥)
        instance_id = instances[0]['ID']
        dicom_file_response = requests.get(
            f"{orthanc_url}/instances/{instance_id}/file",
            auth=(settings.ORTHANC_USERNAME, settings.ORTHANC_PASSWORD)
        )
        dicom_file_response.raise_for_status()

        # 3. pydicomìœ¼ë¡œ íŒŒì‹±
        from io import BytesIO
        dicom_bytes = BytesIO(dicom_file_response.content)
        dicom_dataset = pydicom.dcmread(dicom_bytes)

        # 4. í”½ì…€ ë°ì´í„° ì¶”ì¶œ
        pixel_array = dicom_dataset.pixel_array

        # Hounsfield Unit ë³€í™˜ (CTì˜ ê²½ìš°)
        if dicom_dataset.Modality == 'CT':
            intercept = dicom_dataset.RescaleIntercept
            slope = dicom_dataset.RescaleSlope
            pixel_array = pixel_array * slope + intercept

        # NumPy â†’ List ë³€í™˜ (JSON ì§ë ¬í™”)
        image_list = pixel_array.tolist()

        # 5. DICOM ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata = {
            "StudyInstanceUID": str(dicom_dataset.StudyInstanceUID),
            "SeriesInstanceUID": str(dicom_dataset.SeriesInstanceUID),
            "SOPInstanceUID": str(dicom_dataset.SOPInstanceUID),
            "PatientID": str(dicom_dataset.PatientID),
            "StudyDescription": str(getattr(dicom_dataset, 'StudyDescription', '')),
            "BodyPartExamined": str(getattr(dicom_dataset, 'BodyPartExamined', '')),
            "SliceThickness": float(getattr(dicom_dataset, 'SliceThickness', 0)),
            "KVP": float(getattr(dicom_dataset, 'KVP', 0)),
            "WindowCenter": float(getattr(dicom_dataset, 'WindowCenter', 0)),
            "WindowWidth": float(getattr(dicom_dataset, 'WindowWidth', 0))
        }

        # 6. FastAPIë¡œ ì¶”ë¡  ìš”ì²­
        logger.info(f"Sending inference request to FastAPI: {FASTAPI_BASE_URL}/inference")

        payload = {
            "image_array": [image_list],  # 3D: [1, height, width]
            "modality": str(dicom_dataset.Modality),
            "metadata": metadata,
            "model_id": model_id,
            "options": {
                "return_heatmap": True,
                "confidence_threshold": 0.7
            }
        }

        # HTTP POST ìš”ì²­
        inference_response = requests.post(
            f"{FASTAPI_BASE_URL}/inference",
            json=payload,
            timeout=60  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ
        )
        inference_response.raise_for_status()

        # 7. ê²°ê³¼ íŒŒì‹±
        result = inference_response.json()
        logger.info(f"Inference completed: {result['inference_id']}")

        # 8. Django DBì— ê²°ê³¼ ì €ì¥ (Django REST API í˜¸ì¶œ)
        save_inference_result(study_instance_uid, result)

        return result

    except requests.exceptions.Timeout:
        logger.error(f"FastAPI request timeout for Study {study_instance_uid}")
        raise self.retry(countdown=60, exc=Exception("FastAPI timeout"))

    except requests.exceptions.RequestException as e:
        logger.error(f"FastAPI request failed: {str(e)}")
        raise self.retry(countdown=30, exc=e)

    except Exception as e:
        logger.error(f"Inference task failed: {str(e)}", exc_info=True)
        raise

def save_inference_result(study_uid: str, result: Dict[str, Any]):
    """ì¶”ë¡  ê²°ê³¼ë¥¼ Django DBì— ì €ì¥"""
    django_api_url = settings.DJANGO_API_URL  # "http://django:8000/api"

    payload = {
        "study_instance_uid": study_uid,
        "inference_id": result['inference_id'],
        "model_id": result['model_id'],
        "model_version": result['model_version'],
        "predictions": result['predictions'],
        "metadata": result['metadata']
    }

    response = requests.post(
        f"{django_api_url}/ai/inference-results/",
        json=payload,
        headers={"Authorization": f"Bearer {settings.INTERNAL_API_TOKEN}"}
    )
    response.raise_for_status()
    logger.info(f"Inference result saved to Django: {study_uid}")
```

### 4.2 Django ì„¤ì • (settings.py)

```python
# NeuroNova_02_backend/config/settings/base.py

# FastAPI AI ì„œë²„ URL
FASTAPI_AI_URL = os.environ.get('FASTAPI_AI_URL', 'http://fastapi-ai:8000/api/v1')

# Orthanc PACS ì„¤ì •
ORTHANC_URL = os.environ.get('ORTHANC_URL', 'http://orthanc:8042')
ORTHANC_USERNAME = os.environ.get('ORTHANC_USERNAME', 'orthanc')
ORTHANC_PASSWORD = os.environ.get('ORTHANC_PASSWORD', 'orthanc')

# ë‚´ë¶€ API ì¸ì¦ í† í° (Celery â†’ Django í†µì‹ ìš©)
INTERNAL_API_TOKEN = os.environ.get('INTERNAL_API_TOKEN', 'secret-internal-token')
```

---

## 5. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### 5.1 ëª©í‘œ ì„±ëŠ¥ ì§€í‘œ

| í•­ëª© | ëª©í‘œ | ì¸¡ì • ë°©ë²• |
|------|------|----------|
| **ì¶”ë¡  ì§€ì—°ì‹œê°„** | < 500ms (ë‹¨ì¼ CT ìŠ¬ë¼ì´ìŠ¤) | `metadata.inference_time_ms` |
| **ì „ì²˜ë¦¬ ì‹œê°„** | < 150ms | `metadata.preprocessing_time_ms` |
| **ë™ì‹œ ìš”ì²­ ì²˜ë¦¬** | 10 req/s (GPU 1ê°œ) | Locust ë¶€í•˜ í…ŒìŠ¤íŠ¸ |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰** | < 4GB (ëª¨ë¸ ë¡œë“œ ì‹œ) | `torch.cuda.memory_allocated()` |
| **GPU í™œìš©ë¥ ** | > 80% (ì¶”ë¡  ì¤‘) | `nvidia-smi` |

### 5.2 ë¶€í•˜ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

**íŒŒì¼**: `tests/performance/locustfile.py`

```python
from locust import HttpUser, task, between
import json
import numpy as np

class FastAPILoadTest(HttpUser):
    wait_time = between(1, 3)
    host = "http://localhost:8000"

    @task
    def inference_request(self):
        # ë”ë¯¸ 512x512 CT ì´ë¯¸ì§€
        dummy_image = np.random.rand(512, 512).tolist()

        payload = {
            "image_array": [dummy_image],
            "modality": "CT",
            "metadata": {
                "StudyInstanceUID": "1.2.840.113619.2.55.3.test",
                "SeriesInstanceUID": "1.2.840.113619.2.55.3.test.series",
                "SOPInstanceUID": "1.2.840.113619.2.55.3.test.sop",
                "PatientID": "TEST-001",
                "BodyPartExamined": "BRAIN"
            },
            "model_id": "brain_hemorrhage_v2",
            "options": {
                "return_heatmap": False,
                "confidence_threshold": 0.7
            }
        }

        self.client.post("/api/v1/inference", json=payload)
```

**ì‹¤í–‰**:
```bash
locust -f tests/performance/locustfile.py --users 10 --spawn-rate 2
```

---

## 6. Docker ë°°í¬ ì„¤ì •

### 6.1 Dockerfile

**íŒŒì¼**: `NeuroNova_05_ai_core/Dockerfile`

```dockerfile
FROM pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime

WORKDIR /app

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    && rm -rf /var/lib/apt/lists/*

# Python íŒ¨í‚¤ì§€ ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY ./app /app/app
COPY ./models /models

# FastAPI ì„œë²„ ì‹¤í–‰
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "2"]
```

### 6.2 requirements.txt

```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
torch==2.1.0
torchvision==0.16.0
numpy==1.24.3
scikit-image==0.22.0
pydicom==2.4.3
pillow==10.1.0
python-multipart==0.0.6
prometheus-client==0.19.0
```

### 6.3 docker-compose.yml ì¶”ê°€

**íŒŒì¼**: `docker-compose.dev.yml` (ê¸°ì¡´ íŒŒì¼ì— ì¶”ê°€)

```yaml
services:
  # ... (ê¸°ì¡´ ì„œë¹„ìŠ¤ë“¤)

  fastapi-ai:
    build:
      context: ./NeuroNova_05_ai_core
      dockerfile: Dockerfile
    container_name: neuronova-fastapi-ai
    ports:
      - "8000:8000"
    volumes:
      - ./NeuroNova_05_ai_core/models:/models:ro  # ëª¨ë¸ íŒŒì¼ ì½ê¸° ì „ìš© ë§ˆìš´íŠ¸
      - ./NeuroNova_05_ai_core/app:/app/app  # ê°œë°œ ì¤‘ ì½”ë“œ ìˆ˜ì • ë°˜ì˜
    environment:
      - CUDA_VISIBLE_DEVICES=0  # GPU 0ë²ˆ ì‚¬ìš©
      - LOG_LEVEL=INFO
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - neuronova-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
```

---

## 7. ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### 7.1 Prometheus ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸

**FastAPI êµ¬í˜„**:
```python
from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST
from fastapi import Response

# ë©”íŠ¸ë¦­ ì •ì˜
inference_requests_total = Counter(
    'inference_requests_total',
    'Total inference requests',
    ['model_id', 'status']
)

inference_duration_seconds = Histogram(
    'inference_duration_seconds',
    'Inference duration in seconds',
    ['model_id']
)

@app.get("/api/v1/metrics")
async def metrics():
    """Prometheus ë©”íŠ¸ë¦­ ë…¸ì¶œ"""
    return Response(content=generate_latest(), media_type=CONTENT_TYPE_LATEST)

@app.post("/api/v1/inference", response_model=InferenceResponse)
async def inference(request: InferenceRequest):
    start_time = time.time()

    try:
        # ... ì¶”ë¡  ë¡œì§ ...

        inference_requests_total.labels(model_id=model_id, status='success').inc()
        return result

    except Exception as e:
        inference_requests_total.labels(model_id=model_id, status='error').inc()
        raise

    finally:
        duration = time.time() - start_time
        inference_duration_seconds.labels(model_id=model_id).observe(duration)
```

### 7.2 êµ¬ì¡°í™”ëœ ë¡œê¹…

```python
import logging
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_data = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName
        }

        if record.exc_info:
            log_data["exception"] = self.formatException(record.exc_info)

        return json.dumps(log_data)

# ë¡œê±° ì„¤ì •
handler = logging.StreamHandler()
handler.setFormatter(JSONFormatter())
logging.root.addHandler(handler)
logging.root.setLevel(logging.INFO)
```

---

## 8. ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

### 8.1 ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬

- **FastAPIëŠ” ì™¸ë¶€ ë…¸ì¶œ ê¸ˆì§€**: `docker-compose.yml`ì—ì„œ `ports` ì œê±°
- **Celeryë§Œ ì ‘ê·¼ ê°€ëŠ¥**: Docker ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©
- **Production ì„¤ì •**:
```yaml
fastapi-ai:
  # ports:  # ì œê±°! ì™¸ë¶€ ë…¸ì¶œ ì•ˆ í•¨
  #   - "8000:8000"
  networks:
    - neuronova-internal  # ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬ë§Œ
```

### 8.2 ì…ë ¥ ê²€ì¦

```python
from pydantic import validator

class InferenceRequest(BaseModel):
    image_array: List[List[List[float]]]

    @validator('image_array')
    def validate_image_shape(cls, v):
        if len(v) == 0:
            raise ValueError("image_array cannot be empty")

        # ìµœëŒ€ í¬ê¸° ì œí•œ (DoS ë°©ì§€)
        if len(v) > 100:  # ìµœëŒ€ 100 ìŠ¬ë¼ì´ìŠ¤
            raise ValueError("Too many slices (max 100)")

        height = len(v[0])
        width = len(v[0][0])

        if height > 2048 or width > 2048:
            raise ValueError("Image too large (max 2048x2048)")

        return v
```

### 8.3 Rate Limiting (ì¶”í›„ ê³ ë„í™”)

```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

@app.post("/api/v1/inference")
@limiter.limit("10/minute")  # IPë‹¹ ë¶„ë‹¹ 10íšŒ
async def inference(request: Request, data: InferenceRequest):
    # ...
```

---

## 9. íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ

### 9.1 ìì£¼ ë°œìƒí•˜ëŠ” ì—ëŸ¬

#### CUDA Out of Memory

**ì¦ìƒ**:
```json
{
  "error": "CUDAOutOfMemory",
  "message": "GPU ë©”ëª¨ë¦¬ ë¶€ì¡±"
}
```

**í•´ê²° ë°©ë²•**:
1. `docker-compose.yml`ì—ì„œ GPU ë©”ëª¨ë¦¬ ì œí•œ í™•ì¸
2. ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ (`options.batch_size` ì¤„ì´ê¸°)
3. ëª¨ë¸ ê²½ëŸ‰í™” (INT8 ì–‘ìí™”)
4. ë©€í‹° GPU ë¶„ì‚° ì²˜ë¦¬

#### FastAPI ì„œë²„ ì‘ë‹µ ì—†ìŒ

**ì¦ìƒ**: Celeryì—ì„œ `requests.exceptions.Timeout`

**ì§„ë‹¨**:
```bash
# 1. FastAPI ì»¨í…Œì´ë„ˆ ë¡œê·¸ í™•ì¸
docker logs neuronova-fastapi-ai

# 2. Health Check í™•ì¸
curl http://localhost:8000/api/v1/health

# 3. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í…ŒìŠ¤íŠ¸ (Celery ì»¨í…Œì´ë„ˆ ë‚´ë¶€)
docker exec -it neuronova-celery-worker bash
curl http://fastapi-ai:8000/api/v1/health
```

**í•´ê²° ë°©ë²•**:
- FastAPI ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘: `docker-compose restart fastapi-ai`
- ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ ë¡œê·¸ í™•ì¸: `/models` ë””ë ‰í† ë¦¬ ê¶Œí•œ í™•ì¸

---

## 10. í–¥í›„ ê°œì„  ê³„íš

### Phase 3 (Week 10-12)
- [ ] **ë©€í‹° ìŠ¬ë¼ì´ìŠ¤ ì¶”ë¡ **: 3D CNN ëª¨ë¸ ì§€ì›
- [ ] **ë°°ì¹˜ ì¶”ë¡  ìµœì í™”**: TorchScript, ONNX Runtime
- [ ] **ëª¨ë¸ ë²„ì „ ê´€ë¦¬**: MLflow í†µí•©
- [ ] **A/B í…ŒìŠ¤íŠ¸**: ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê¸°ëŠ¥

### Phase 4 (Week 13-16)
- [ ] **AutoML íŒŒì´í”„ë¼ì¸**: ìë™ ëª¨ë¸ ì¬í•™ìŠµ
- [ ] **Explainability**: SHAP, Integrated Gradients
- [ ] **Federation Learning**: ë³‘ì› ê°„ ëª¨ë¸ ê³µìœ  (í”„ë¼ì´ë²„ì‹œ ë³´í˜¸)

---

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

- [06_ì‹œìŠ¤í…œ_ì•„í‚¤í…ì²˜_v3.md](./06_ì‹œìŠ¤í…œ_ì•„í‚¤í…ì²˜_v3.md) - ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜
- [45_DICOM_ë·°ì‰_ì‹œí€€ìŠ¤_ë‹¤ì´ì–´ê·¸ë¨.md](./45_DICOM_ë·°ì‰_ì‹œí€€ìŠ¤_ë‹¤ì´ì–´ê·¸ë¨.md) - Secure Proxy íŒ¨í„´
- [10_API_ëª…ì„¸ì„œ.md](./10_API_ëª…ì„¸ì„œ.md) - Django REST API ì „ì²´ ëª…ì„¸
- [ì•„í‚¤í…ì²˜_ì •ë°€_ì ê²€_ë³´ê³ ì„œ_20260102.md](./90_ì‘ì—…ì´ë ¥/ì•„í‚¤í…ì²˜_ì •ë°€_ì ê²€_ë³´ê³ ì„œ_20260102.md) - ì•„í‚¤í…ì²˜ ë¦¬ë·° ê²°ê³¼

---

**ì‘ì„±**: Claude AI (Sonnet 4.5)
**ì‘ì„±ì¼**: 2026-01-02
**ìƒíƒœ**: âœ… í™•ì • (v3.0)
